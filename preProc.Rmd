---
title: "Preprocessing Stream Occurrence Data"
author: "Josh Erickson"
date: "April 14, 2020"
output: html_document
---

```{r setup, include=FALSE}

library(sf)
library(raster)
library(terra)
library(caret)
library(CAST)
library(blockCV)
library(nhdplusTools)
library(corrplot)
library(tidyverse)
```

Now bring in all the TIFs to be used in the analysis and stack/visualise. You'll need to download these from [google drive](https://drive.google.com/drive/folders/100in8JlxDCbRLTPCzQ-p9D54SQxIP37d?usp=sharing) and put into a folder.  

```{r, eval=FALSE}

twi30 <- rast("D:/R_folder/Rasters/Common/twi30agg.tif") #TWI

vvsd30 <- rast("D:/R_folder/Rasters/Common/vvsd30agg.tif") #vertical vertical sd

vv30 <- rast("D:/R_folder/Rasters/Common/vv30agg.tif") #vertical vertical mean

npol30 <- rast("D:/R_folder/Rasters/Common/npol30agg.tif") #normalized polarization 

accum30 <- rast("D:/R_folder/Rasters/Common/accum30.tif") #UAA d-infinity aggregated from 10-m

nppMid30 <- rast("D:/R_folder/Rasters/Common/nppmmid30agg.tif") #NPP median '86-18'

deficit <- rast("D:/R_folder/Rasters/Common/deficitRS.tif") # annual CWD 30 yr

wtrbdy30 <- rast("D:/R_folder/Rasters/Common/wtrbdy30agg.tif") #waterbodies

tpi30 <- rast("D:/R_folder/Rasters/Common/tpi30agg.tif") #TPI

CAD <- terra::rast("D:/R_folder/Rasters/Common/cad30RS.tif") #cold air drainage

decid <- rast("D:/R_folder/Rasters/Common/decid30RS.tif") #deciduous

Blue <- rast("D:/R_folder/Rasters/Common/Blue_med.tif") #blue (B2)

Green <- rast("D:/R_folder/Rasters/Common/Green_med.tif") #green (B3)

Red <- rast("D:/R_folder/Rasters/Common/Red_med.tif") #red (B4)

NIR <- rast("D:/R_folder/Rasters/Common/NIR_med.tif") #Near Infrared (B8)

NDVI <-  rast("D:/R_folder/Rasters/Common/NDVI_med.tif")
  
NDWI <-  rast("D:/R_folder/Rasters/Common/NDWI_med.tif")

cpgPrecip <- rast("D:/R_folder/Rasters/Common/cpg30precip.tif") #Continuous parameter grid (precipitation/PRISM).

topo_opt_rad34 <- rast(list(twi30, tpi30, accum30,vv30, vvsd30, npol30,  NDVI, NDWI, nppMid30, deficit, CAD, decid, Blue, Green, Red, NIR, cpgPrecip))
```

Now visualise.

```{r, echo=FALSE}

plot(topo_opt_rad34, maxnl=32, nc = 4, legend = FALSE)

```


Bring in the points collected in the field.

```{r, eval=FALSE}

#combine point objects
proj.study <- "+proj=aea +lat_1=46 +lat_2=48 +lat_0=44 +lon_0=-109.5 +x_0=600000 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"

pts34 <- read_sf("data/hw_study.gpkg", layer = "points_spaced30m")  #points to the folder/feature

st_crs(pts34) <- proj.study

pts34 <- st_transform(pts34, crs = proj.study)

#clean up

pts34 <- pts34[,c('copy_TWI_1')]

```


Now bring in the applicable polygons for clipping, etc. Just use the final products in the `hw_study.gpkg`. This is just showing the processs. 
```{r, eval=FALSE}

#Get forest service boundary
FSland <- read_sf("data/KNF_ownership.shp")
FSland <- FSland[FSland$OWNER == 'FS',]

FSland <- st_transform(FSland, proj.study)

#Get District Boundary
DistrictBoundary <- read_sf("data/district_boundary.shp")[1]

DistrictBoundary <- st_transform(DistrictBoundary, proj.study)
#now intersect
landClip <- st_intersection(FSland, DistrictBoundary)

#read in the landClip we created and call it FSland
FSland <- landClip

plot(FSland[1])

##st_write(landClip, 'hw_study.gpkg', "LandClip", driver = "ESRI Shapefile", update = TRUE)
##st_write(DistrictBoundary, 'hw_study.gpkg', "DistrictBoundary", driver = "ESRI Shapefile")
```

Now we'll need to bring in the NHDPlus layer and then rasterize so that we can get NHDPlus baseline data. You'll need to download to a tempfile or file path of choice if you want to reproduce it otherwise just use the final `nhdStrOrdRast.tif`.

```{r, eval=FALSE}

##download_dir <- download_nhdplushr(nhd_dir = getwd(), c("1701"))

nhdPlushr <- get_nhdplushr(getwd(), "nhdplus.gpkg",
layers = "NHDFlowline", overwrite = TRUE, proj = "+proj=aea +lat_1=46 +lat_2=48 +lat_0=44 +lon_0=-109.5 +x_0=600000 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")

nhdDistrict <- nhdPlushr$NHDFlowline

nhdDistrict <- st_transform(nhdDistrict, "+proj=aea +lat_1=46 +lat_2=48 +lat_0=44 +lon_0=-109.5 +x_0=600000 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")

nhdDistrict <- st_crop(nhdDistrict, c(xmin = 150035.8, xmax = 222365.8, 
                                      ymin = 507390, ymax = 574500 ))

plot(nhdDistrict["StreamOrde"], main = "NHDPlus High Res with Stream Order")

nhdDistrictSF <- st_as_sf(nhdDistrict)
st_write(nhdDistrictSF, "hw_study.gpkg", "nhdDistrictSF")
```

This just reads in the data we created from above and rasterizes it.

```{r, eval = FALSE}
nhdDistrictSF <- st_read("hw_study.gpkg", "nhdDistrictSF")
#now make a 30m raster with streamorder

ndhStrOrdRast <- rasterize(vect(nhdDistrict), tpi30, field = "StreamOrde")

writeRaster(ndhStrOrdRast, "data/nhdStrOrdRast.tiff", overwrite = TRUE)

nhdStrOrdRast <- rast("data/nhdStrOrdRast.tif")

```

Now check some stats for the sample area real quick.

```{r}
#sample space area sq/km
area_sum <- sum(st_area(FSland$geometry))

area_km <- area_sum/1000000

area_km
```

Now add that NHDPlus raster to the main raster stack and perform the masking out of wetlands and FSland. Then finally extract points from stack `thrty.34` and then combined with `pts34`. Create two different dataframes so that we can have (long, lat) and (utm's).

```{r, eval = FALSE}
#add nhd raster
topo_opt_rad34 <- c(topo_opt_rad34, nhdStrOrdRast)

thrty.34 <- terra::mask(topo_opt_rad34, wtrbdy30, inverse = TRUE) #mask out waterbodies if you want

thrty.34 <- terra::mask(thrty.34, terra::vect(FSland)) #mask out non-Forest Service land

thrty.34 <- terra::extract(thrty.34, vect(pts34)) #extract the values from raster by point data

top34.opt.rad <- cbind(pts34, thrty.34)

top34.opt.rad2 <- cbind(pts34, thrty.34)

#get long lat for graphing/visualizing
top34.opt.rad2 <- st_transform(top34.opt.rad2, crs = "+proj=longlat +datum=WGS84 +no_defs")

top34coords <- do.call(rbind, st_geometry(top34.opt.rad)) %>% 
    as_tibble() %>% setNames(c("utm1","utm2"))

top34lonlat <- do.call(rbind, st_geometry(top34.opt.rad2)) %>% 
    as_tibble() %>% setNames(c("lon","lat"))

st_geometry(top34.opt.rad) <- NULL

top34.opt.rad <- data.frame(cbind(top34.opt.rad,top34coords, top34lonlat))

write.csv(top34.opt.rad, file = "data/top34.opt.rad.csv")
```

Now clean up the data. Pre-proccessing.  

```{r, eval = FALSE}
top34.opt.rad <- read.csv("data/top34.opt.rad.csv", header = TRUE)[-1]

data34 <- top34.opt.rad #change for ease

#change names
data34 <- data34 %>% rename(stream = 'copy_TWI_1', 
                            NHDPlus = 'nhdStrOrdRast')

data34$stream <- factor(data34$stream)

#make NHDPlus "NA/land" areas = 0.
data34$NHDPlus[is.na(data34$NHDPlus)] <- 0

#remove any missing data after the previous step.
data34 <- na.omit(data34)

#this only takes streams less than stream order = 3
data34 <- data34 %>% filter(NHDPlus < 3)

# write csv

write.csv(data34, file = "data/data34.csv")

```

Now we can start looking at some of the data visually.

```{r, eval = FALSE}
data34 <- read_csv("data/data34.csv")[-1]

summary(data34)

data34 %>% dplyr::select(Blue_med, Red_med, Green_med, NIR_med)  %>% pivot_longer(cols = everything(), names_to = "Feature", values_to = "value") %>% ggplot() + geom_boxplot(aes(value, color = Feature)) + coord_flip()



```

```{r echo=FALSE, fig.align='center'}

data34 %>% mutate(accum30 = log(accum30)) %>% dplyr::select(npol30agg, accum30) %>% pivot_longer(cols = everything(), names_to = "Feature", values_to = "value") %>% ggplot() + geom_boxplot(aes(value)) + coord_flip() + facet_wrap(~Feature, scales = "free_y")






```


```{r, eval = FALSE}
data34 <- data34 %>% filter(npol30agg > -1, npol30agg < 1)
```

Now look at the new boxplot and histogram for `npol30agg`, which looks much better.

```{r echo=FALSE, fig.align='center'}
#now look at boxplot
data34 %>% select(npol30agg) %>% gather(key, value) %>% ggplot() + geom_boxplot(aes(value)) + coord_flip()
#much better
```

After visualising, we can take a deeper look and search the data frame for correlation using a certain threshold (e.g., 90%). This is possible by  `caret`'s function called `findCorrelation`. This is the cutoff threshold that @hird2017google and @lidberg2020using used.

```{r fig.align='center', eval = FALSE}

correlations34 <- cor(data34 %>% select(twi30agg:cpg30precip)) #remove response and coords and NHDPlus
corrplot(correlations34, order = "hclust")

highCorr34 <- findCorrelation(correlations34, cutoff = 0.9)
highCorr34
```



There is one variable recommended to be taken out. Let's see what they are. 

```{r}
colnames(correlations34)[c(13,15,7)]
```

We ended up taking out NDWI_med instead of NDVI_med along with Blue_med and Red_med.
```{r}
data34 <- data34[, -which(names(data34) %in% c("NDWI_med", "Blue_med", "Red_med"))]

```

```{r, out.width= "70%", fig.align='center', message=FALSE}

barplot(prop.table(table(data34$stream)),
        main = "Proportion of stream occurence", 
        names.arg = c("No Stream", "Stream"), ylab = "Proportion %") 
```
```{r, eval=TRUE, echo=FALSE}
kableExtra::kable(addmargins(table(Stream = data34$stream)), caption = "Count")
kableExtra::kable(round(prop.table(table(Stream = data34$stream)),3), caption = "Proportion")
```
We ended up having a more imbalanced data set then we wanted. 

So we can downsample the higher target class. Always make sure to do after pre-processing to avoid any leakage.

```{r}

data34$stream <- factor(data34$stream)

levels(data34$stream) <- c("X1", "X2")

set.seed(1234)

data34 <- downSample(data34, data34$stream)


write_csv(data34, "data/data34.csv", append = FALSE)
```

```{r, out.width= "70%", fig.align='center', message=FALSE}

barplot(prop.table(table(data34$stream)),
        main = "Proportion of stream occurence", 
        names.arg = c("No Stream", "Stream"), ylab = "Proportion %") 

```

Now we want to bring in our point data along with our covariates and clean up all the preprocessing we did. 
```{r}

data34 <- read_csv("data/data34.csv")

ptsSF34 <- st_as_sf(data34, coords = c("utm1","utm2"))

st_crs(ptsSF34) <- proj.study

topo_new34 <- topo_opt_rad34[[-which(names(topo_opt_rad34) %in% c("NDWI_med", "Blue_med", "Red_med"))]] #covariate stack

crs(topo_new34) <- proj.study
```

Now we can figure out our 'effective range of spatial autocorrelation' by using the function `spatialAutoRange` in `blockCV`. This is a cool function because it takes the covariates, calculates the range of autocorrelation (from $n$ sampled points), and then produces two plots showing the range of the covariates and the recommended block size for validation.  

```{r}

sac34 <- blockCV::spatialAutoRange(rasterLayer = topo_new34,
                                 sampleNumber = 5000,
                                 doParallel = TRUE,
                                 showPlots = TRUE)

```

```{r}

Spatial_Med34 <- blockCV::cv_spatial(x = ptsSF34, # presence-background data
                    column = "stream",
                    r = topo_new34,
                    k = 10,
                    selection = "systematic",
                    biomod2 = TRUE)

```

We noticed though that when doing this type of blocking strategy the standard deviation between hold outs is very high, which can lead to bias-variance issues when modeling. See below after adding k-means nearsest neighbor.  


Finally, we can do a `kmeans` with a cluster of 80. This will be our large structure attempt.

```{r}
#this is the k-means method (using a cluster of 80)
#need coordinates.
#remember to use the coordinates, that's why we've kept them!
data34 <- data34[,-21] #take out the class added when we downsampled
write_csv(data34, "data/data34.csv")
Mycluster80 <- kmeans(data34[,c(17,18)], (nrow(data34)/80)) 

# add the new variable back to your dataframe here
data34$spatial_cluster80 = Mycluster80$cluster

ptsSF34$spatial_cluster80 = data34$spatial_cluster80
```

now we can do a `kmeans` with a cluster of 40. This will be our small structure attempt.


```{r}
#this is the k-means method (using a cluster of 40)
#need coordinates.
#remember to use the coordinates, that's why we've kept them!

Mycluster40 <- kmeans(data34[,c(17,18)], (nrow(data34)/40)) 

# add the new variable back to your dataframe here
data34$spatial_cluster40 = Mycluster40$cluster

ptsSF34$spatial_cluster40 = data34$spatial_cluster40
```

Now add the spatial block medium.

```{r}
data34$sb_med <- Spatial_Med34$folds_ids
```

```{r}
write_csv(data34, "data/data34.csv")
```


It's important to note that this method is very subjective (40, 80) and sub-optimal (blocking). The work-around we 
chose to do was to do a lot of hold-outs. You can see this method in `ffs_script_clustering`, which is the final method we decided to do.

