---
title: "dataSplit"
author: "Josh Erickson"
date: "April 15, 2020"
output: html_document
---

```{r setup, include=FALSE}

load("D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/workingthroughproj.RData")
library(sp)
library(rgdal)
library(rgeos)
library(raster)
library(automap)
library(caret)
library(CAST)
library(blockCV)
library(tidyverse)
library(gridExtra)
library(reshape2)
library(doParallel)
library(parallel)
library(mlbench)
library(plotROC)
library(pdp)
library(vip)
library(gridGraphics)
library(grid)
```

## Model Selection  

**Model Splitting**  

 Above in the data splitting section I  proposed that we wanted to split the data into three partitions with the test data being the final model performance measure (sanity check). To do this we will split the data into two chunks and then split into test proportions once our final model is selected. This will hopefully give us a *natural* statistic into the final model performance. 

```{r, eval = FALSE}
library(caret)
library(CAST)



set.seed(1234)
#split data into training and test data. 90% for training is the goal. 10% for sanity check. 
dat34index <- createDataPartition(data34$stream, list = FALSE, p=0.90)

train <- data34[dat34index,]

test <- data34[-dat34index,]

#then split the training data into two so we have 'overall' 40% for feature selection and 'overall' 60% for training.
set.seed(1234)
trainindex <- createDataPartition(train$stream, list = FALSE, p = .6)

traintune <- train[trainindex,]

trainsel <- train[-trainindex,]

testSF <- st_as_sf(test, coords = c("coords.x1","coords.x2"))

st_crs(testSF) <- "+proj=aea +lat_1=46 +lat_2=48 +lat_0=44 +lon_0=-109.5 +x_0=600000 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"
```

Now check and see the final sanity check data. 
```{r}
plot(testSF["stream"])
```

Looks good.



Another important step in evaluating spatial data is accounting for spatial autocorrelation. We can do this by doing spatial cross validation where folds are created at random; however, when specified (i.e., HUCs, block, cluster) these folds are systematically seperated by k-folds [@roberts2017cross; @valavi2018blockcv]. This will help correct for any spatial autocorrelation in the validation process, which will lead to more realistic results [@meyer2018improving; @meyer2019importance]. The goal is to see how model performances react with different dependance structures. Hence finding a balance between 'too large' and 'too small.' Below we will use the `CAST` package to index these dependance structures. 

```{r, eval=FALSE}
library(CAST)

#this is the index for the HUC12 method.
set.seed(1234)
indices12sel <- CreateSpacetimeFolds(trainsel, spacevar = "sb12", k = 10)

set.seed(1234)
indices12tune <- CreateSpacetimeFolds(traintune,spacevar = "sb12", k = 10) 

#now for the blockCV method

set.seed(1234)
indicesMedsel <- CreateSpacetimeFolds(trainsel, spacevar = "sbMed", k = 10)

set.seed(1234)
indicesMedtune <- CreateSpacetimeFolds(traintune,spacevar = "sbMed", k = 10) 

#this is the index for the HUC14 method

set.seed(1234)
indices14sel <- CreateSpacetimeFolds(trainsel, spacevar = "sb14", k = 10)

set.seed(1234)
indices14tune <- CreateSpacetimeFolds(traintune,spacevar = "sb14", k = 10) 

#now for the cluster method

set.seed(1234)
indicesKsel <- CreateSpacetimeFolds(trainsel, spacevar = "spatial_cluster", k = 10)

set.seed(1234)
indicesKtune <- CreateSpacetimeFolds(traintune,spacevar = "spatial_cluster", k = 10) 
 
```

