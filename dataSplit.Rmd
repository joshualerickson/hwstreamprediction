---
title: "dataSplit"
author: "Josh Erickson"
date: "April 15, 2020"
output: html_document
---

```{r setup, include=FALSE}

library(caret)
library(CAST)
library(sf)
```

## Model Selection  

**Model Splitting**  

```{r, eval = FALSE}

set.seed(1234)

data34 <- downSample(data34, data34$stream)

set.seed(1234)
#split data into training and test data. 75% for training and validation. 25% for feature selection. 
dat34index <- createDataPartition(data34$stream, list = FALSE, p=0.75)


train <- data34[dat34index,]

FeatureSelection <- data34[-dat34index,]


```

Another important step in evaluating spatial data is accounting for spatial autocorrelation. We can do this by doing spatial cross validation where folds are created at random; however, when specified (i.e., HUCs, block, cluster) these folds are systematically seperated by k-folds [@roberts2017cross; @valavi2018blockcv]. This will help correct for any spatial autocorrelation in the validation process, which will lead to more realistic results [@meyer2018improving; @meyer2019importance]. The goal is to see how model performances react with different dependance structures. Hence finding a balance between 'too large' and 'too small.' Below we will use the `CAST` package to index these dependance structures. 

```{r, eval=FALSE}
library(CAST)

set.seed(1234)
indicesKsel80 <- CreateSpacetimeFolds(FeatureSelection, spacevar = "spatial_cluster80", k = 10)

set.seed(1234)
indicesKselLLO80 <- CreateSpacetimeFolds(FeatureSelection, spacevar = "spatial_cluster80", k = 120)


set.seed(1234)
indicesKtrain80 <- CreateSpacetimeFolds(train,spacevar = "spatial_cluster80", k = 10) 

set.seed(1234)
indicesKtrainLLO80 <- CreateSpacetimeFolds(train,spacevar = "spatial_cluster80", k = 120) 

```

